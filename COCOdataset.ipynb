{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marzooklathief/deep-learning/blob/main/COCOdataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqZHYEkbl-Co",
        "outputId": "466856ca-7592-42cb-94c7-254c8b528633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (2.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Install the necessary libraries\n",
        "!pip install torch torchvision matplotlib pycocotools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAwLiU1ImCLs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torchvision.transforms import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKcCvMRsmIVC",
        "outputId": "0d853d7b-f14a-41e1-d7d3-f3fe1faf677f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-10-10 17:47:35--  http://images.cocodataset.org/zips/train2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 54.231.166.9, 52.216.41.177, 3.5.30.146, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|54.231.166.9|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19336861798 (18G) [application/zip]\n",
            "Saving to: ‘train2017.zip.1’\n",
            "\n",
            "train2017.zip.1     100%[===================>]  18.01G  55.1MB/s    in 5m 58s  \n",
            "\n",
            "2024-10-10 17:53:34 (51.5 MB/s) - ‘train2017.zip.1’ saved [19336861798/19336861798]\n",
            "\n",
            "--2024-10-10 17:53:34--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 16.15.194.125, 3.5.17.230, 3.5.25.44, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|16.15.194.125|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘annotations_trainval2017.zip.1’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.19M  52.0MB/s    in 10s     \n",
            "\n",
            "2024-10-10 17:53:44 (23.1 MB/s) - ‘annotations_trainval2017.zip.1’ saved [252907541/252907541]\n",
            "\n",
            "replace train2017/000000147328.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "# Download the COCO dataset using wget (Train 2017 images + annotations)\n",
        "!wget http://images.cocodataset.org/zips/train2017.zip\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "\n",
        "# Unzip the dataset\n",
        "!unzip -q train2017.zip\n",
        "!unzip -q annotations_trainval2017.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wk9B9R2fmVAE"
      },
      "outputs": [],
      "source": [
        "class CocoDataset(CocoDetection):\n",
        "    def __init__(self, root, annFile, transform=None):\n",
        "        super().__init__(root, annFile)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, targets = super().__getitem__(idx)\n",
        "\n",
        "        # Convert the image to a tensor\n",
        "        img = F.to_tensor(img)\n",
        "\n",
        "        if self.transform:\n",
        "            img, targets = self.transform(img, targets)\n",
        "\n",
        "        return img, targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y06cgUFUmXq5"
      },
      "outputs": [],
      "source": [
        "# Define dataset paths\n",
        "train_img_dir = 'train2017/'\n",
        "train_ann_file = 'annotations/instances_train2017.json'\n",
        "\n",
        "# Create COCO dataset and DataLoader\n",
        "train_dataset = CocoDataset(root=train_img_dir, annFile=train_ann_file)\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPicDirWmcP5"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained Faster R-CNN with ResNet-50 backbone\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "# Change the number of classes (COCO has 80 classes + background)\n",
        "num_classes = 91  # 80 classes + background\n",
        "\n",
        "# Modify the model's classifier to fit the COCO dataset\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "# Utility function to print debug information about data batches\n",
        "def debug_batch_structure(images, targets, batch_idx):\n",
        "    print(f\"Batch {batch_idx}:\")\n",
        "    print(f\"  Type of images: {type(images)}\")\n",
        "    print(f\"  Type of targets: {type(targets)}\")\n",
        "\n",
        "    if isinstance(targets, tuple):\n",
        "        print(f\"  Targets are a tuple with {len(targets)} elements.\")\n",
        "        for idx, t in enumerate(targets):\n",
        "            print(f\"    Target {idx} type: {type(t)} - {t}\")\n",
        "    elif isinstance(targets, list):\n",
        "        print(f\"  Targets are a list with {len(targets)} elements.\")\n",
        "        for idx, t in enumerate(targets):\n",
        "            print(f\"    Target {idx} type: {type(t)} - {t}\")\n",
        "    else:\n",
        "        print(\"  Targets are of an unknown format.\")\n",
        "\n",
        "# Move the target tensors to the correct device\n",
        "def move_targets_to_device(targets, device):\n",
        "    if isinstance(targets, list):\n",
        "        return [{k: v.to(device) if torch.is_tensor(v) else v for k, v in t.items()} for t in targets]\n",
        "    elif isinstance(targets, dict):\n",
        "        return {k: v.to(device) if torch.is_tensor(v) else v for k, v in targets.items()}\n",
        "    return targets\n",
        "\n",
        "# Function for training one epoch\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10):\n",
        "    model.train()  # Set model to training mode\n",
        "    metric_logger = {'loss': 0}\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch_idx, (images, targets) in enumerate(data_loader):\n",
        "        # Debugging: print batch structure for the first 5 batches\n",
        "        if batch_idx < 5:\n",
        "            debug_batch_structure(images, targets, batch_idx)\n",
        "\n",
        "        # Move images and targets to the device\n",
        "        images = [image.to(device) for image in images]\n",
        "        targets = move_targets_to_device(targets, device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        loss_dict = model(images, targets)\n",
        "\n",
        "        # Compute the total loss\n",
        "        total_loss = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update loss metrics\n",
        "        metric_logger['loss'] += total_loss.item()\n",
        "\n",
        "        # Print training stats periodically\n",
        "        if (batch_idx + 1) % print_freq == 0:\n",
        "            print(f\"Epoch {epoch}, Batch {batch_idx + 1}, Loss: {total_loss.item():.4f}\")\n",
        "\n",
        "    print(f\"Epoch {epoch} training complete. Total Loss: {metric_logger['loss']:.4f}\")\n",
        "    return metric_logger\n",
        "\n",
        "# Main training loop\n",
        "def train_model(model, train_loader, num_epochs=10, lr=0.005, momentum=0.9, weight_decay=0.0005, step_size=3, gamma=0.1):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Set up the optimizer and learning rate scheduler\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Starting epoch {epoch + 1}/{num_epochs}\")\n",
        "        train_one_epoch(model, optimizer, train_loader, device, epoch + 1)\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "# Example usage\n",
        "# train_model(model, train_loader)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dv2SyQ0P23sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Evaluate the model on a few images\n",
        "model.eval()\n",
        "images, targets = next(iter(train_loader))\n",
        "\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device) for img in images])\n",
        "\n",
        "# Plot the results\n",
        "def plot_results(images, prediction):\n",
        "    for img, pred in zip(images, prediction):\n",
        "        plt.imshow(img.permute(1, 2, 0).cpu())\n",
        "        boxes = pred['boxes'].cpu().numpy()\n",
        "        scores = pred['scores'].cpu().numpy()\n",
        "        labels = pred['labels'].cpu().numpy()\n",
        "\n",
        "        # Display boxes and labels\n",
        "        for box, score, label in zip(boxes, scores, labels):\n",
        "            if score > 0.5:  # Thresholding based on confidence\n",
        "                plt.gca().add_patch(plt.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=2, edgecolor='r', facecolor='none'))\n",
        "                plt.text(box[0], box[1], f'{label}: {score:.2f}', color='red', fontsize=12)\n",
        "        plt.show()\n",
        "\n",
        "plot_results(images, prediction)\n"
      ],
      "metadata": {
        "id": "a85uOryPwggt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsz2ZMWNm5v4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f594f43-20f5-4578-d109-cd71e9a4fb0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.99s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "from pycocotools.coco import COCO\n",
        "\n",
        "# Load the COCO validation dataset\n",
        "val_img_dir = 'val2017/'\n",
        "val_ann_file = 'annotations/instances_val2017.json'\n",
        "\n",
        "val_dataset = CocoDataset(root=val_img_dir, annFile=val_ann_file)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}